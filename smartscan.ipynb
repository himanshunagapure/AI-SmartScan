{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5114b4e0-d6a7-4052-af53-8beb51bf60e0",
   "metadata": {},
   "source": [
    "### Get The text from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bddfa6f2-b9f1-406f-ae60-1276da12e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfplumber pytesseract pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949ff88c-66a0-4c31-9e79-fed2f3ca4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ad7c35-584e-4655-9e66-be124d3e680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        # Try direct text extraction\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text\n",
    "\n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Direct text extraction failed: {e}\")\n",
    "\n",
    "    # Fallback to OCR for image-based PDFs\n",
    "    print(\"Falling back to OCR for image-based PDF.\")\n",
    "    try:\n",
    "        images = convert_from_path(pdf_path)\n",
    "        for image in images:\n",
    "            page_text = pytesseract.image_to_string(image)\n",
    "            text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed: {e}\")\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f999ebf-1a00-4832-a45f-7490fdcebc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Text from PDF:\n",
      "TEJAS GUPTA\n",
      "Faridabad,Haryana|9971297789|Gmail| LinkedIn |Portfolio\n",
      "Proficient in Data Visualization, Reporting, Dashboarding, Insight analysis with Computer Science Engineering background with\n",
      "hands on experience in SQL, Excel, and Power BI. Honing strong analytical skills, decision-making, problem solving abilities with\n",
      "learningmindsetandproactiveapproach.\n",
      "TECHNICALSKILLS\n",
      "• PowerBIDesktop • AdvancedExcel • Python • DataModelling\n",
      "• PowerBIService • SQL • Streamlit • DataVisualization\n",
      "• PowerQuery • MySQL • MachineLearning • DataCleaning\n",
      "• DAX • SSMS • Tableau • DataStorytelling\n",
      "WORKEXPERIENCE\n",
      "DataAnalystIntern|EYGDS-AICTE 02/2024–04/2024\n",
      "• UtilizedPowerBItoanalyzehealthcaredata,contributingtoenhancedpatientcareandstreamlinedoperationsthroughactionable\n",
      "insights.\n",
      "• Analyzedpatientandadmissiondatatoidentifytrends,visualizekeymetrics,andprovideinsightsondiseaseanalysisbasedon\n",
      "smokingandalcoholhabits,patientdemographics,andhospitalstaydurations,leadingtoevidence-basedstrategiesforhealthcare\n",
      "management.\n",
      "Toolsused:PowerBIdesktop,Powerquery,DAXfunction\n",
      "MachineLearningIntern|XebiaITArchitectsPvtLtd 06/2023–07/2023\n",
      "• ConductedwebscrapingonFlipkart.com,collectedandanalyzedover30,000productreviewstoperformsentimentanalysisusing\n",
      "NLPtechniques,classifyingreviewsasPositive,Negative,orNeutral.\n",
      "• DevelopedandoptimizedanXGBoostmodel,achievinganaccuracyof74%inpredictingreviewpolarity.\n",
      "Toolsused:Python,BeautifulSoup,VSCode,JupyterNotebook\n",
      "PROJECTS\n",
      "CustomerChurnAnalysis|SQL,PowerBI,Python|Link\n",
      "• Conductedin-depthanalysisoftelecomcustomerdata,focusingondemographic,geographic,payment,andserviceattributesto\n",
      "identifychurnpatternsandcustomerprofiles.\n",
      "• BuiltanefficientETLframeworkinSQLanddesignedaPowerBIdashboardwithcustomDAXmeasurestovisualizechurn\n",
      "trends.Integrated84%-accuratechurnpredictionsfromaPython-basedRandomForestmodel.\n",
      "• Predicted378churnerswitha27%churnrate,drivingdata-backedinsightsthatidentifiedkeychurndriversandoptimizedtargeted\n",
      "marketingandretentionstrategies.\n",
      "CreditCardFinancialDashboard|PowerBI|Link\n",
      "• Developedacomprehensivecreditcardweeklydashboardtoprovidereal-timeinsightsintokeyperformancemetricsandtrends.\n",
      "• Integratedandprepared datafrommultipledatasetsfromSQLdatabase,createdcalculatedcolumnsandmeasures,anddesigned\n",
      "interactivevisualizationsinPowerBI.\n",
      "• Resultedina28.8%increaseinweek-on-weekrevenue, withoverallYTDrevenuereaching$57million.\n",
      "WhatsappChatAnalysis|Python|Link\n",
      "• DevelopedaPython-basedtoolforanalyzingWhatsAppchatexports,providingreal-timeinsightsintocommunicationpatternsat\n",
      "bothuserandgrouplevelsanddeployedtheapponStreamlitCloud.\n",
      "ParticulateMatterPredictionUsingTimeSeries|Python|Link\n",
      "• DevelopedarobusttimeseriesmodeltoforecastPM2.5andPM10levels,enablingproactiveairqualitymanagement.\n",
      "• ConductedEDAanddatapreprocessingonreal-timedatafromJanuary1,2018toDecember31,2023,andtrainedmultiple\n",
      "models,identifyingLSTMasthebestperformerbasedonevaluationmetricsanddeployedthemodelonStreamlitCloud.\n",
      "E-commerceSalesDashboard|Excel|Tableau|Link\n",
      "• DevelopedaninteractiveTableaudashboardforane-commercecompanytoanalyzesalestrendsandshippingdaysbyproduct\n",
      "category,featuringproductcategoryselectionandmonth-wiseanalysis.\n",
      "• UtilizedExceltobuildhistograms,comboboxes,andcolumncharts,optimizingthedashboardforauser-friendlyexperience.\n",
      "EDUCATION\n",
      "B.TechCSEwithspecializationinArtificialIntelligenceandMachineLearning 10/2020–11/2024\n",
      "ManavRachnaUniversity CGPA:7.73\n",
      "ACHIEVEMENTS\n",
      "• MicrosoftCertified:PowerBIDataAnalystAssociate.Link\n",
      "• HackerRank–SQLGoldBadge.Link.\n",
      "• AccentureDataAnalyticsVirtualExperience-Forage.Link\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Tejas Gupta Analyst.pdf\"\n",
    "resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "print(\"\\nExtracted Text from PDF:\")\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d590c819-80d9-4165-a1ff-757f9c85ada1",
   "metadata": {},
   "source": [
    "### Set Google GenerativeAI Api Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa02a451-8ad3-4100-b05c-d1dda658d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google.generativeai\n",
      "  Downloading google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\himan\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google.generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google.generativeai)\n",
      "  Downloading google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google.generativeai)\n",
      "  Downloading google_api_python_client-2.160.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google.generativeai)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\himan\\anaconda3\\lib\\site-packages (from google.generativeai) (3.20.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\himan\\anaconda3\\lib\\site-packages (from google.generativeai) (1.10.20)\n",
      "Requirement already satisfied: tqdm in c:\\users\\himan\\anaconda3\\lib\\site-packages (from google.generativeai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\himan\\anaconda3\\lib\\site-packages (from google.generativeai) (4.11.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading proto_plus-1.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google.generativeai)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from google-api-core->google.generativeai) (2.32.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google.generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google.generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google.generativeai)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google.generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google.generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\himan\\anaconda3\\lib\\site-packages (from tqdm->google.generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai)\n",
      "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google.generativeai) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\himan\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2024.7.4)\n",
      "Collecting protobuf (from google.generativeai)\n",
      "  Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Downloading google_generativeai-0.8.4-py3-none-any.whl (175 kB)\n",
      "   ---------------------------------------- 0.0/175.4 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 30.7/175.4 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 92.2/175.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  174.1/175.4 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 175.4/175.4 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.3 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.4/1.3 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.5/1.3 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.7/1.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.2/1.3 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.3/1.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
      "   ---------------------------------------- 0.0/160.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 160.1/160.1 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "   ---------------------------------------- 0.0/210.8 kB ? eta -:--:--\n",
      "   -------------------------------------- - 204.8/210.8 kB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 210.8/210.8 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading google_api_python_client-2.160.0-py2.py3-none-any.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.8 MB 8.9 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.3/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.6/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.6/12.8 MB 5.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.0/12.8 MB 3.8 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.2/12.8 MB 3.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.5/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.8/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.0/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.3/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.5/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.7/12.8 MB 4.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.9/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.2/12.8 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.4/12.8 MB 4.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.1/12.8 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.6/12.8 MB 4.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.8/12.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.4/12.8 MB 5.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.8/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.0/12.8 MB 5.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.4/12.8 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.1/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 8.3/12.8 MB 5.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.8 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.9/12.8 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.3/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.3/12.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.1/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.1/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.1/12.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.2/12.8 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.8/12.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.2/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.6/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.7/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.0/12.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.2/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.4/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.4/12.8 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.5/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.7/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "   ---------------------------------------- 0.0/221.7 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 153.6/221.7 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 221.7/221.7 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.9/96.9 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.26.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.2/50.2 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.3 MB 7.9 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.4/4.3 MB 5.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.5/4.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.7/4.3 MB 4.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.9/4.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.1/4.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.3/4.3 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 1.5/4.3 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.6/4.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.9/4.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.0/4.3 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.2/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.4/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.6/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.8/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.0/4.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.2/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.6/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.7/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.2/4.3 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.2/4.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.70.0-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "   ---------------------------------------- 0.0/434.5 kB ? eta -:--:--\n",
      "   ---------------- ---------------------- 184.3/434.5 kB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 348.2/434.5 kB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  430.1/434.5 kB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 434.5/434.5 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, grpcio, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google.generativeai\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.24.1 google-api-python-client-2.160.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google.generativeai-0.8.4 googleapis-common-protos-1.66.0 grpcio-1.70.0 grpcio-status-1.70.0 httplib2-0.22.0 proto-plus-1.26.0 protobuf-5.29.3 rsa-4.9 uritemplate-4.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install google.generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d65dc91-1852-49c7-90e7-7547ca121b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2186307e-11c7-4a65-86ac-936d2a3f02d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"The capital of India is **New Delhi**.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"avg_logprobs\": -0.0026347806677222254\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 8,\n",
       "        \"candidates_token_count\": 10,\n",
       "        \"total_token_count\": 18\n",
       "      },\n",
       "      \"model_version\": \"gemini-1.5-flash\"\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate_content(\"What is the capital of India?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a26b5d9c-e78b-4121-ad70-845a50f5e20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is **New Delhi**.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c4b89-3cd7-400d-b239-93a42e3956af",
   "metadata": {},
   "source": [
    "### Resume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84d37cf5-f349-4960-91ed-3368d25584d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_resume(resume_text, job_description=None):\n",
    "    if not resume_text:\n",
    "        return {\"error\": \"Resume text is required for analysis.\"}\n",
    "    \n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    \n",
    "    base_prompt = f\"\"\"\n",
    "    You are an experienced HR with Technical Experience in the field of any one job role from Data Science, Data Analyst, DevOPS, Machine Learning Engineer, Prompt Engineer, AI Engineer, Full Stack Web Development, Big Data Engineering, Marketing Analyst, Human Resource Manager, Software Developer your task is to review the provided resume.\n",
    "    Please share your professional evaluation on whether the candidate's profile aligns with the role. Also mention Skills he already have and suggest some skills to imporve his resume , also suggest some course he might take to improve the skills. Highlight the strengths and weaknesses.\n",
    "\n",
    "    Resume:\n",
    "    {resume_text}\n",
    "    \"\"\"\n",
    "\n",
    "    if job_description:\n",
    "        base_prompt += f\"\"\"\n",
    "        Additionally, compare this resume to the following job description:\n",
    "        \n",
    "        Job Description:\n",
    "        {job_description}\n",
    "        \n",
    "        Highlight the strengths and weaknesses of the applicant in relation to the specified job requirements. Give the percentage of match if the resume matches\n",
    "        the job description. First the output should come as percentage and then keywords missing and last final thoughts.\n",
    "        \"\"\"\n",
    "\n",
    "    response = model.generate_content(base_prompt)\n",
    "\n",
    "    analysis = response.text.strip()\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52d947c-17ad-4411-b6cd-6a8d2ee8abcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Resume Review: Tejas Gupta\\n\\n**Role Focus:**  Assuming the target role is a Data Analyst (given the resume content), this review focuses on that.  My experience is in Data Science and overlaps significantly with Data Analyst roles.\\n\\n**Overall Assessment:** Tejas Gupta\\'s resume demonstrates a promising start to a data analyst career.  His projects showcase a good understanding of data analysis techniques and tool utilization. However, there are areas for improvement in terms of quantifying achievements and showcasing more advanced skills.\\n\\n**Strengths:**\\n\\n* **Diverse Skillset:** He possesses a solid foundation in essential data analysis tools (SQL, Power BI, Excel, Python), data visualization, and data storytelling.\\n* **Practical Experience:**  Internships at EY GDS-AICTE and Xebia IT Architects provide valuable real-world experience. The projects further strengthen this practical application.\\n* **Project Portfolio:**  Multiple projects demonstrate initiative and a willingness to learn and apply new techniques.  The variety of tools and datasets used is impressive for someone at this stage.\\n* **Certifications and Achievements:** The Microsoft Power BI Data Analyst Associate certification and HackerRank Gold Badge in SQL are significant accomplishments that bolster credibility.\\n* **Strong Communication (on resume):** The resume is well-structured and clearly communicates his skills and experiences.\\n\\n\\n**Weaknesses:**\\n\\n* **Quantifiable Results:** While the projects mention positive outcomes (e.g., revenue increase), they lack specific numerical quantification.  Instead of \"28.8% increase in week-on-week revenue,\" it should state \"Increased week-on-week revenue by 28.8%, resulting in $X million in additional revenue.\"  This applies to all projects.  Focus on the impact - what did *you* contribute, in numbers?\\n* **Limited Depth in Machine Learning:**  The ML internship and project involving sentiment analysis demonstrate exposure, but the 74% accuracy needs context (compared to what baseline?). Deeper dives into ML models and techniques would strengthen his profile for roles requiring more advanced analytics.\\n* **Lack of Advanced Analytical Techniques:** The resume emphasizes descriptive analytics.  Demonstrating proficiency in predictive modeling (beyond simple models like Random Forest and XGBoost), statistical testing (hypothesis testing, A/B testing), and causal inference would significantly enhance his competitiveness.\\n* **Resume Length:**  While concise, the resume could benefit from expanding on accomplishments within each role and project, providing more context and quantifiable results.\\n\\n\\n**Skills:**\\n\\n**Existing Skills:** SQL, Power BI (Desktop & Service), Power Query, DAX, Advanced Excel, Python (including libraries like BeautifulSoup), Data Visualization, Data Storytelling, Data Cleaning, Data Modeling, Machine Learning (basic), Streamlit, Tableau, MySQL, SSMS, NLP (basic).\\n\\n**Skills to Improve:**\\n\\n* **Advanced Statistical Modeling:** Regression analysis (linear, logistic, etc.), time series analysis (ARIMA, Prophet), clustering, dimensionality reduction.\\n* **Data Mining and Feature Engineering:** Techniques to extract meaningful features from raw data, handle missing values, and improve model performance.\\n* **Cloud Computing (AWS, Azure, GCP):**  Familiarity with cloud platforms is increasingly important for data analysts.\\n* **Big Data Technologies (Spark, Hadoop):**  For handling and processing large datasets.\\n* **Data Warehousing and ETL Processes:**  In-depth knowledge of data warehousing concepts and ETL tools.\\n* **Advanced Data Visualization:**  Interactive dashboards, data storytelling with compelling narratives.\\n* **Version Control (Git):** Essential for collaborative projects.\\n\\n\\n**Course Suggestions:**\\n\\n* **Coursera/edX:**  Courses on statistical modeling, machine learning, cloud computing, and big data technologies.  Specific courses should target the \"Skills to Improve\" list above.\\n* **DataCamp/Udacity:**  Interactive courses focusing on practical application of data analysis tools and techniques.\\n* **Kaggle:** Participating in competitions and working on publicly available datasets will provide valuable experience and enhance the portfolio.\\n\\n\\n**Recommendations:**\\n\\n1. **Quantify Achievements:**  Revise the resume to include specific numerical results and impact metrics for each project and internship.\\n2. **Expand on Projects:**  Provide more details on the methodology, challenges faced, and solutions implemented within each project.\\n3. **Highlight Advanced Skills (if any):** If Tejas has experience with more advanced techniques, make sure they are prominently featured.\\n4. **Add a Summary:**  A brief summary at the beginning highlighting key skills and career goals would be beneficial.\\n5. **Tailor to Specific Roles:** Customize the resume for each job application, emphasizing skills and experiences relevant to the specific role requirements.\\n\\n\\nBy addressing these weaknesses and expanding on his strengths, Tejas can significantly improve his resume and increase his chances of securing a data analyst position.  The foundation is strong; refining the presentation and broadening the skillset will make a big difference.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_resume(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbf69d-20c8-4aaa-9a16-739db7aa1994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
